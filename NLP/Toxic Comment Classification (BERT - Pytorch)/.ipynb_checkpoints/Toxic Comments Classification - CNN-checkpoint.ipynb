{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:56.314509Z",
     "start_time": "2020-04-08T04:42:55.359967Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:56.340402Z",
     "start_time": "2020-04-08T04:42:56.315466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:56.348411Z",
     "start_time": "2020-04-08T04:42:56.341396Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=20)\n",
    "SEED = 2020\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:56.353390Z",
     "start_time": "2020-04-08T04:42:56.349377Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/train.csv\"\n",
    "PREPROCESS_DATA_DIR = \"data/preprocessed.csv\"\n",
    "TEST_DIR = \"data/test.csv\"\n",
    "PREPROCESS_TEST_DIR = \"data/preprocessed_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.024623Z",
     "start_time": "2020-04-08T04:42:56.354361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "traindata = pd.read_csv(DATA_DIR)\n",
    "print(traindata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.029583Z",
     "start_time": "2020-04-08T04:42:57.025566Z"
    }
   },
   "outputs": [],
   "source": [
    "MAKE_PREPROCESSED_DATA = 0\n",
    "TRAIN_DATA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.744746Z",
     "start_time": "2020-04-08T04:42:57.031578Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "spacy_tokenizer = torchtext.data.utils.get_tokenizer('spacy')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocessing(text):\n",
    "  \n",
    "    def tokenizer(text):\n",
    "        text = str.split(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_punctuations(sentence):\n",
    "        result = \"\".join([w if w not in punctuations and not w.isdigit() else \" \" for w in sentence])\n",
    "        return result\n",
    "    \n",
    "    def word_lemmatizer(sentence):\n",
    "        result = lemmatizer.lemmatize(sentence)\n",
    "        return result\n",
    "    \n",
    "    def word_lowercase(sentence):\n",
    "        return sentence.lower()\n",
    "    \n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        html=re.compile(r'<.*?>')\n",
    "        text = html.sub(r'',text)\n",
    "        text = url.sub(r'',str(text))\n",
    "        return text\n",
    "  \n",
    "    def remove_newline(text):\n",
    "        return text.rstrip(\"\\n\")\n",
    "    \n",
    "    def clean(sentence):\n",
    "        result = []\n",
    "        sentence = remove_newline(sentence)\n",
    "        sentence = remove_URL(sentence)\n",
    "        sentence = word_lowercase(sentence)\n",
    "        sentence = word_lemmatizer(sentence)\n",
    "        sentence = remove_punctuations(sentence)\n",
    "        sentence = tokenizer(sentence)\n",
    "\n",
    "        result = \" \".join(sentence)\n",
    "        return result\n",
    "    \n",
    "    #result = generate_bigrams(result)   \n",
    "    text = clean(text)\n",
    "    if text == \"\":\n",
    "        text = \"None\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.751625Z",
     "start_time": "2020-04-08T04:42:57.745640Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "if MAKE_PREPROCESSED_DATA:\n",
    "    with open(TEST_DIR, \"r\", encoding=\"utf8\") as in_csv, open(PREPROCESS_TEST_DIR, \"w\", newline=\"\", encoding=\"utf8\") as out_csv:\n",
    "        reader = csv.reader(in_csv)\n",
    "        writer = csv.writer(out_csv)\n",
    "        next(reader, None) # Skip header\n",
    "        for row in tqdm(reader):\n",
    "            row[1] = preprocessing(row[1])\n",
    "            try:\n",
    "                writer.writerow(row)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.757709Z",
     "start_time": "2020-04-08T04:42:57.752622Z"
    }
   },
   "outputs": [],
   "source": [
    "def mytokenizer(sentence):\n",
    "    tokens = str.split(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:42:57.766585Z",
     "start_time": "2020-04-08T04:42:57.758605Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first = True,\n",
    "                  tokenize = mytokenizer,\n",
    "                  stop_words=stopwords_list)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "ID = data.LabelField()\n",
    "ID2 = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:43:06.219083Z",
     "start_time": "2020-04-08T04:42:57.767581Z"
    }
   },
   "outputs": [],
   "source": [
    "FIELDS = [[\"id\",ID], [\"text\", TEXT], [\"toxic\",LABEL],[\"s_toxic\",LABEL],\n",
    "          [\"obscene\",LABEL],[\"threat\",LABEL],[\"insult\",LABEL],[\"id_hate\",LABEL]]\n",
    "TEST_FIELDS = [[\"id\",ID2], [\"text\", TEXT]]\n",
    "\n",
    "dataset = data.TabularDataset(PREPROCESS_DATA_DIR,\n",
    "                              format = \"csv\",\n",
    "                              fields=FIELDS,\n",
    "                              skip_header=True)\n",
    "\n",
    "testset = data.TabularDataset(PREPROCESS_TEST_DIR,\n",
    "                              format= \"csv\",\n",
    "                              fields=TEST_FIELDS,\n",
    "                             skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:43:06.223963Z",
     "start_time": "2020-04-08T04:43:06.219973Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '000103f0d9cfb60f', 'text': ['aww', 'matches', 'background', 'colour', 'seemingly', 'stuck', 'thanks', 'talk', 'january', 'utc'], 'toxic': '0', 's_toxic': '0', 'obscene': '0', 'threat': '0', 'insult': '0', 'id_hate': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(dataset.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:43:06.390546Z",
     "start_time": "2020-04-08T04:43:06.224961Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 127657\n",
      "Number of validating samples: 31914\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "trainset, valset = dataset.split(split_ratio=0.8, random_state=random.seed(SEED))\n",
    "print(f\"Number of training samples: {len(trainset)}\")\n",
    "print(f\"Number of validating samples: {len(valset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:43:06.394536Z",
     "start_time": "2020-04-08T04:43:06.391514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequential': False, 'use_vocab': True, 'init_token': None, 'eos_token': None, 'unk_token': '<unk>', 'fix_length': None, 'dtype': torch.int64, 'preprocessing': None, 'postprocessing': None, 'lower': False, 'tokenizer_args': (None, 'en'), 'tokenize': <function _split_tokenizer at 0x000001F0F56733A8>, 'include_lengths': False, 'batch_first': False, 'pad_token': None, 'pad_first': False, 'truncate_first': False, 'stop_words': None, 'is_target': False}\n"
     ]
    }
   ],
   "source": [
    "print(vars(ID2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:43:37.676814Z",
     "start_time": "2020-04-08T04:43:34.300828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training set: 40002\n",
      "Unique labels in training set: 2\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 40000\n",
    "TEXT.build_vocab(trainset,\n",
    "                 min_freq = 3,\n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trainset)\n",
    "ID.build_vocab(trainset)\n",
    "ID2.build_vocab(testset)\n",
    "print(f\"Unique words in training set: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique labels in training set: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:02.458644Z",
     "start_time": "2020-04-08T04:45:02.434710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00001cee341fdb12', 1), ('0000247867823ef7', 1), ('00013b17ad220c46', 1), ('00017563c3f7919a', 1), ('00017695ad8997eb', 1), ('0001ea8717f6de06', 1), ('00024115d4cbde0f', 1), ('000247e83dcc1211', 1), ('00025358d4737918', 1), ('00026d1092fe71cc', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(ID2.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:02.464629Z",
     "start_time": "2020-04-08T04:45:02.459613Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits((trainset, valset),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  device = device)\n",
    "\n",
    "test_iter = data.BucketIterator(testset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=False,\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:02.474574Z",
     "start_time": "2020-04-08T04:45:02.466595Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.752220Z",
     "start_time": "2020-04-08T04:45:02.475575Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(40002, 100, padding_idx=1)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 6\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-4,weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.758194Z",
     "start_time": "2020-04-08T04:45:03.753207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,122,306 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.772184Z",
     "start_time": "2020-04-08T04:45:03.759191Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.777168Z",
     "start_time": "2020-04-08T04:45:03.773154Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.785122Z",
     "start_time": "2020-04-08T04:45:03.779147Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels(batch):\n",
    "    toxic = batch.toxic.unsqueeze(1)\n",
    "    s_toxic = batch.s_toxic.unsqueeze(1)\n",
    "    obscene = batch.obscene.unsqueeze(1)\n",
    "    threat = batch.threat.unsqueeze(1)\n",
    "    insult = batch.insult.unsqueeze(1)\n",
    "    id_hate = batch.id_hate.unsqueeze(1)\n",
    "    labels = torch.cat((toxic,s_toxic,obscene,\n",
    "                        threat,insult,id_hate),dim=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.793125Z",
     "start_time": "2020-04-08T04:45:03.787117Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_step(model, optimizer, criterion, batch):\n",
    "    batch_size = len(batch)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    text = batch.text.view(batch_size, -1)\n",
    "    labels = get_labels(batch)\n",
    "\n",
    "    outputs = model(text)\n",
    "    loss = criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.800082Z",
     "start_time": "2020-04-08T04:45:03.794098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_DATA:\n",
    "    EPOCHS = 3\n",
    "    loss_list = []\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            train_loss = train_step(model,optimizer, criterion, batch)\n",
    "\n",
    "            if i%400 == 0:\n",
    "                print(f\"Epoch: [{epoch+1}/{EPOCHS}] | Iterations: [{i+1}/{len(train_iter)}] | Training loss: {train_loss:.3f}\")\n",
    "                torch.save(model.state_dict(), \"model/modelCNN.pt\")\n",
    "        #loss_list.append(train_loss)\n",
    "    print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.824044Z",
     "start_time": "2020-04-08T04:45:03.801079Z"
    }
   },
   "outputs": [],
   "source": [
    "if not TRAIN_DATA:\n",
    "    model.load_state_dict(torch.load(\"model/modelCNN.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:03.830026Z",
     "start_time": "2020-04-08T04:45:03.825015Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_test(model, test_iter):\n",
    "    result = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            batch_size = len(batch)\n",
    "            text = batch.text.view(batch_size,-1).long()\n",
    "            ids = batch.id.squeeze().cpu()\n",
    "            output = model(text)\n",
    "            output = torch.sigmoid(output).cpu()\n",
    "            for i,j in zip(ids,output):\n",
    "                result.append([ID2.vocab.itos[i.numpy()],j.numpy()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:24.771025Z",
     "start_time": "2020-04-08T04:45:03.831000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2394/2394 [00:20<00:00, 114.41it/s]\n"
     ]
    }
   ],
   "source": [
    "result = predict_test(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:24.777008Z",
     "start_time": "2020-04-08T04:45:24.772049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9878593  0.04721861 0.90824836 0.00333032 0.73442763 0.04421253]\n"
     ]
    }
   ],
   "source": [
    "print(result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:45:25.755419Z",
     "start_time": "2020-04-08T04:45:24.778007Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 153164/153164 [00:00<00:00, 158323.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"submission.csv\", \"w\", newline=\"\", encoding=\"utf8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\"]+[\"toxic\"]+[\"severe_toxic\"]+[\"obscene\"]+[\"threat\"]+[\"insult\"]+[\"identity_hate\"])\n",
    "    for line in tqdm(result):\n",
    "        writer.writerow([line[0]] + [line[1][i] for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:50:29.682041Z",
     "start_time": "2020-04-08T04:50:29.225295Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:50:30.938139Z",
     "start_time": "2020-04-08T04:50:30.934149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo bitch ja rule is more succesful then you ll ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time\n"
     ]
    }
   ],
   "source": [
    "text = preprocessing(\"yo bitch ja rule is more succesful then you ll ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:50:32.340069Z",
     "start_time": "2020-04-08T04:50:32.330095Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.99372923374176025391, 0.11364216357469558716, 0.95650070905685424805,\n",
      "         0.00829244125634431839, 0.85854005813598632812, 0.08795237541198730469]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
